{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='green'>JOBLÄ°B</font> \n",
    "\n",
    "Joblib is a set of tools to provide lightweight pipelining in Python. In particular:\n",
    "\n",
    "1. transparent disk-caching of functions and lazy re-evaluation (memoize pattern)\n",
    "2. easy simple parallel computing\n",
    "\n",
    "Joblib is optimized to be **fast** and **robust** on large data in particular and has specific optimizations for numpy arrays. It is **BSD-licensed.**\n",
    "\n",
    "Joblib provides a simple helper class to write parallel for loops using multiprocessing. The core idea is to write the code to be executed as a generator expression, and convert it to parallel computing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exe_time-sec:  0.0\n"
     ]
    }
   ],
   "source": [
    "from math import sqrt\n",
    "import time\n",
    "start_t = time.time()\n",
    "[sqrt(i ** 2) for i in range(10)]\n",
    "print(\"exe_time-sec: \", time.time()-start_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "can be spread over 2 CPUs using the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exe_time-sec:  0.2472999095916748\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "start_t = time.time()\n",
    "Parallel(n_jobs=2)(delayed(sqrt)(i ** 2) for i in range(10))\n",
    "print(\"exe_time-sec: \", time.time()-start_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default **joblib.Parallel** uses the **'loky'** backend module to start separate Python worker processes to execute tasks concurrently on separate CPUs. This is a reasonable default for generic Python programs but can induce a significant overhead as the input and output data need to be serialized in a queue for communication with the worker processes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you know that the function you are calling is based on a compiled extension that releases the Python Global Interpreter Lock (GIL) during most of its computation then it is more efficient to use threads instead of Python processes as concurrent workers. For instance this is the case if you write the CPU intensive part of your code inside a with nogil block of a Cython function.\n",
    "\n",
    "To hint that your code can efficiently use threads, just pass prefer=\"threads\" as parameter of the joblib.Parallel constructor. In this case joblib will automatically use the \"threading\" backend instead of the default \"loky\" backend:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exe_time-sec:  0.11553597450256348\n"
     ]
    }
   ],
   "source": [
    "start_t = time.time()\n",
    "Parallel(n_jobs=2, prefer=\"threads\")(delayed(sqrt)(i ** 2) for i in range(10))\n",
    "print(\"exe_time-sec: \", time.time()-start_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shared-memory semantics\n",
    "\n",
    "The default backend of joblib will run each function call in isolated Python processes, therefore they cannot mutate a common Python object defined in the main program.\n",
    "\n",
    "However if the parallel function really needs to rely on the shared memory semantics of threads, it should be made explicit with require='sharedmem', for instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shared_set = set()\n",
    "def collect(x):\n",
    "    shared_set.add(x)\n",
    "\n",
    "Parallel(n_jobs=2, require='sharedmem')(delayed(collect)(i) for i in range(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(shared_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep in mind that relying a on the shared-memory semantics is probably suboptimal from a performance point of view as concurrent access to a shared Python object will suffer from lock contention.m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ref : \n",
    "https://joblib.readthedocs.io/en/latest/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='green'>Examples for Joblib</font>\n",
    "\n",
    "General examples : General-purpose and introductory examples for joblib.\n",
    "\n",
    "### Random state within joblib.Parallel\n",
    "\n",
    "Randomness is affected by parallel execution differently by the different backends.\n",
    "\n",
    "In particular, when using multiple processes, the random sequence can be the same in all processes. This example illustrates the problem and shows how to work around it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A utility function for the example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_vector(vector, backend):\n",
    "    \"\"\"Helper function to print the generated vector with a given backend.\"\"\"\n",
    "    print('\\nThe different generated vectors using the {} backend are:\\n {}'\n",
    "          .format(backend, np.array(vector)))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sequential behavior\n",
    "\n",
    "*stochastic_function* will generate five random integers. When calling the function several times, we are expecting to obtain different vectors. For instance, we will call the function five times in a sequential manner, we can check that the generated vectors are all different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The different generated vectors in a sequential manner are:\n",
      " [[5 9 4 1 5]\n",
      " [7 1 0 8 2]\n",
      " [7 5 0 7 7]\n",
      " [0 2 1 7 4]\n",
      " [4 3 1 5 8]]\n"
     ]
    }
   ],
   "source": [
    "def stochastic_function(max_value):\n",
    "    \"\"\"Randomly generate integer up to a maximum value.\"\"\"\n",
    "    return np.random.randint(max_value, size=5)\n",
    "\n",
    "\n",
    "n_vectors = 5\n",
    "random_vector = [stochastic_function(10) for _ in range(n_vectors)]\n",
    "print('\\nThe different generated vectors in a sequential manner are:\\n {}'\n",
    "      .format(np.array(random_vector)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parallel behavior\n",
    "\n",
    "Joblib provides three different backend: loky (default), threading, and multiprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The different generated vectors using the loky backend are:\n",
      " [[9 5 4 4 3]\n",
      " [6 3 4 9 9]\n",
      " [3 3 6 9 1]\n",
      " [3 4 9 2 7]\n",
      " [2 0 3 4 4]]\n"
     ]
    }
   ],
   "source": [
    "backend = 'loky'\n",
    "random_vector = Parallel(n_jobs=2, backend=backend)(delayed(\n",
    "    stochastic_function)(10) for _ in range(n_vectors))\n",
    "print_vector(random_vector, backend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The different generated vectors using the threading backend are:\n",
      " [[1 8 7 3 7]\n",
      " [6 2 0 5 9]\n",
      " [8 7 7 4 7]\n",
      " [2 1 4 9 9]\n",
      " [0 8 2 3 4]]\n"
     ]
    }
   ],
   "source": [
    "backend = 'threading'\n",
    "random_vector = Parallel(n_jobs=2, backend=backend)(delayed(\n",
    "    stochastic_function)(10) for _ in range(n_vectors))\n",
    "print_vector(random_vector, backend)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loky and the threading backends behave exactly as in the sequential case and do not require more care. However, this is not the case regarding the multiprocessing backend.\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "backend = 'multiprocessing'\n",
    "random_vector = Parallel(n_jobs=2, backend=backend)(delayed(\n",
    "    stochastic_function)(10) for _ in range(n_vectors))\n",
    "print_vector(random_vector, backend)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the generated vectors are exactly the same, which can be a problem for the application.\n",
    "\n",
    "Technically, the reason is that all forked Python processes share the same exact random seed. As a results, we obtain twice the same randomly generated vectors because we are using n_jobs=2. A solution is to set the random state within the function which is passed to joblib.Parallel."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def stochastic_function_seeded(max_value, random_state):\n",
    "    rng = np.random.RandomState(random_state)\n",
    "    return rng.randint(max_value, size=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stochastic_function_seeded accepts as argument a random seed. We can reset this seed by passing None at every function call. In this case, we see that the generated vectors are all different."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "random_vector = Parallel(n_jobs=2, backend=backend)(delayed(\n",
    "    stochastic_function_seeded)(10, None) for _ in range(n_vectors))\n",
    "print_vector(random_vector, backend)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ref : \n",
    "https://www.tutorialdocs.com/tutorial/joblib/examples.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
